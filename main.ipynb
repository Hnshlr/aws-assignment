{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cloud Computing Assignment 2022-2023\n",
    "Implementation of an application processing large data sets in parallel on a distributed Cloud environment (ie. AWS)\n",
    "\n",
    "© Copyright 2022, All rights reserved to Hans Haller, CSTE-CIDA Student at Cranfield Uni. SATM, Cranfield, UK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Solution setup - Pre-requisites:\n",
    "1. Make sure the aws credentials taken from the Learner Lab are updated in the ~/.aws/credentials file (Test connection locally using aws sts get-caller-identity)\n",
    "2. Specify the \"labsuser.pem\" perm-key's (taken from the Learner Lab) path, needed by paramiko to connect to the EC2 instances and execute ssh commands.\n",
    "3. Create EC2, S3 and SQS resources and clients using boto3.\n",
    "### Solution setup steps (Using Boto3):\n",
    "1. Create a cluster of EC2 instances on AWS, using the AWS Linux 2 images.\n",
    "2. Create a S3 bucket to store the data.\n",
    "3. Create a SQS queue to store stacks of messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### IMPORTS:\n",
    "\n",
    "The following controllers defines functions that use boto3 packaged functions to make AWS API calls. By importing the controllers, a Boto3 resource is automatically created for each element of the solution (EC2, SQS, SSM, S3, etc) in order for these functions to work. The Boto3 resources uses the AWS credentials that are located in the .aws local folder of the user who executes this software. As a result, it is important that they are updated before running the following. Thus please make sure to restart the kernel and re-execute the imports if the credentials expired (ie. the Learner Lab session ended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from backend.controllers.boto3_controller import *\n",
    "from backend.controllers.ec2_controller import *\n",
    "from backend.controllers.matrix_controller import *\n",
    "from backend.controllers.s3_controller import *\n",
    "from backend.controllers.spark_controller import *\n",
    "from backend.controllers.sqs_controller import *\n",
    "from backend.controllers.ssm_controller import *\n",
    "from backend.controllers.app_controller import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from backend.work_service import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## AWS - SOLUTION SETUP AND TASKS EXECUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SETTINGS=\n",
    "worker_amount = 2\n",
    "backend_path = os.path.join(os.getcwd(), 'backend')     # !! IMPORTANT: Make sure to update this path to the backend folder of the project !!\n",
    "\n",
    "# NAMES=\n",
    "instances_names = np.concatenate((np.array(['master']), np.array(['worker' + str(i) for i in range(1, worker_amount+1)]))).tolist()\n",
    "queues_names = ['main-protected-jobs.fifo', 'main-protected-results.fifo']\n",
    "bucket_name = 'main-protected-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EC2 INSTANCES:\n",
    "create_instances_and_wait_for_running(instances_names)\n",
    "# SQS QUEUES:\n",
    "create_sqs_queues(queues_names)\n",
    "# S3 BUCKET:\n",
    "create_s3_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(get_instance_public_dns_by_name(instances_names[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# UPLOAD BACKEND FOLDER -> S3 BUCKET:\n",
    "upload_dir_to_s3(backend_path, bucket_name, 'backend')\n",
    "for instance_name in instances_names:\n",
    "    # DOWNLOAD BACKEND FOLDER : S3 BUCKET -> EC2 INSTANCES:\n",
    "    download_directory_on_instance_from_s3_bucket(instance_name, bucket_name, 'backend', 'backend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EC2 INSTANCES - SETUP:\n",
    "for instance_name in instances_names:\n",
    "    exec_SSH_on_instance(instance_name, 'pip3 install boto3')\n",
    "    exec_SSH_on_instance(instance_name, 'pip3 install numpy')\n",
    "    update_instance_credentials_using_boto3_session_credentials(instance_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "create_matrix_then_split_and_send_jobs(500, queues_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CREATE A MATRIX - GIVING THE SIDE SIZE:\n",
    "matrix_shape = 500\n",
    "matrix = create_random_square_matrix(matrix_shape)\n",
    "\n",
    "# SPLIT MATRIX INTO BLOCKS - USING OPTIMAL BLOCKS AMOUNT:\n",
    "max_SQS_msg_size = get_max_message_size_from_sqs_queue(queues_names[0])\n",
    "blocks = split_matrix_in_blocks(matrix, find_optimal_blocks_amount(matrix, max_SQS_msg_size))\n",
    "\n",
    "# STORE IN AN ARRAY THE SLICES REQUIRED TO COMPUTE EACH BLOCK OF THE RESULT MATRIX:\n",
    "slices = np.empty((blocks.shape[0], blocks.shape[1], 3), dtype=np.ndarray)\n",
    "for i in range(0, blocks.shape[0]):\n",
    "    for j in range(0, blocks.shape[1]):\n",
    "        slices[i][j][0] = np.concatenate([blocks[i][k] for k in range(blocks.shape[0])], axis=1)\n",
    "        slices[i][j][1] = np.concatenate([blocks[k][j] for k in range(blocks.shape[0])], axis=0)\n",
    "\n",
    "# CREATE A LIST OF MESSAGES TO SEND TO SQS QUEUE:\n",
    "messages = []\n",
    "for i in range(0, blocks.shape[0]):\n",
    "    for j in range(0, blocks.shape[1]):\n",
    "        message_body = {\n",
    "            'i': i,\n",
    "            'j': j,\n",
    "            'left-slice': slices[i][j][0].tolist(),\n",
    "            'right-slice': slices[i][j][1].tolist()\n",
    "        }\n",
    "        json_message_body = json.dumps(message_body)\n",
    "        messages.append(json_message_body)\n",
    "\n",
    "# BULK SEND MESSAGES TO THE \"JOBS\" QUEUE:\n",
    "send_bulk_messages_to_sqs_queue(queues_names[0], messages, 'work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXECUTE THE WORK SERVICE - JOB N°1:\n",
    "for instance_name in instances_names[1:2]:\n",
    "    command = 'python3 backend/work_service.py job1 ' + queues_names[0] + ' ' + queues_names[1]\n",
    "    stdout, stderr = exec_SSH_on_instance(instance_name, command)\n",
    "    print('STDOUT: ', stdout)\n",
    "    print('STDERR: ', stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# command = 'python3 backend/work_service.py job2 ' + queues_names[1] + ' ' + str(blocks.shape[0])\n",
    "# stdout, stderr = exec_SSH_on_instance(instances_names[0], command)\n",
    "print('STDOUT:\\n', stdout.decode('utf-8'))\n",
    "print('STDERR:\\n', stderr.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The result matrix is equal to the self computed product: ' + str(np.array_equal(result_matrix, np.dot(matrix, matrix))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CLEAN-UP:\n",
    "for queue_name in queues_names:\n",
    "    purge_queue(queue_name)\n",
    "    delete_sqs_queue(queue_name)\n",
    "for instances_name in instances_names:\n",
    "    stop_instance_by_name(instances_name)\n",
    "    terminate_instance_by_name(instances_name)\n",
    "delete_s3_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
